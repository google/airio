{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8z0_Pfmf9Gc"
      },
      "source": [
        "# AirIO Inspection Example\n",
        "\n",
        "This notebook demonstrates the creation of a basic `Task` with two\n",
        "preprocessing steps. It performs the following actions:\n",
        "\n",
        "1. Load the [IMDB reviews][imdb_reviews] dataset.\n",
        "2. Map the raw data to a format suitable for training.\n",
        "3. Tokenize the text using SeqIO's\n",
        "[`SentencePieceVocabulary`][seqio_vocabularies].\n",
        "\n",
        "The task's `get_dataset_by_step()` method is called, to demonstrate the contents\n",
        "of each record after each individual processing step.\n",
        "\n",
        "By default, the number of records to inspect at each step is set to 2. Any value\n",
        "between 1 - 1000 can be used.\n",
        "\n",
        "\n",
        "[imdb_reviews]: https://www.tensorflow.org/datasets/catalog/imdb_reviews\n",
        "[seqio_vocabularies]: https://github.com/google/seqio/blob/main/seqio/vocabularies.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6vygnQ8f1Sz"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "import airio\n",
        "from airio.grain import dataset_providers\n",
        "from airio.grain import preprocessors as grain_preprocessors_lib\n",
        "from seqio import vocabularies\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVbsINmNgv8W"
      },
      "source": [
        "Load a SeqIO vocabulary for tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsZEdIm9gmBA"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SPM_PATH = \"gs://t5-data/vocabs/mc4.250000.100extra/sentencepiece.model\"\n",
        "DEFAULT_VOCAB = vocabularies.SentencePieceVocabulary(DEFAULT_SPM_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_PDy_3hg4Y9"
      },
      "source": [
        "Define a method for converting raw data to a format suitable for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uymdrDA9hJ_L"
      },
      "outputs": [],
      "source": [
        "def _imdb_preprocessor(raw_example: Dict[str, bytes]) -\u003e Dict[str, str]:\n",
        "    final_example = {\"inputs\": \"imdb \" + raw_example[\"text\"].decode(\"utf-8\")}\n",
        "    raw_label = str(raw_example[\"label\"])\n",
        "    if raw_label == \"0\":\n",
        "      final_example[\"targets\"] = \"negative\"\n",
        "    elif raw_label == \"1\":\n",
        "      final_example[\"targets\"] = \"positive\"\n",
        "    else:\n",
        "      final_example[\"targets\"] = \"invalid\"\n",
        "    return final_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB8wMB4ohMJ8"
      },
      "source": [
        "Create a simple task with 2 preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ElLmYNchQ5t"
      },
      "outputs": [],
      "source": [
        "task = dataset_providers.GrainTask(\n",
        "      name=\"dummy_airio_task\",\n",
        "      source=airio.data_sources.TfdsDataSource(\n",
        "          tfds_name=\"imdb_reviews/plain_text:1.0.0\", splits=[\"train\"]\n",
        "      ),\n",
        "      preprocessors=[\n",
        "          grain_preprocessors_lib.MapFnTransform(_imdb_preprocessor),\n",
        "          grain_preprocessors_lib.MapFnTransform(\n",
        "              airio.tokenizer.Tokenizer(\n",
        "                  tokenizer_configs={\n",
        "                      \"inputs\": airio.tokenizer.TokenizerConfig(\n",
        "                          vocab=DEFAULT_VOCAB\n",
        "                      ),\n",
        "                      \"targets\": airio.tokenizer.TokenizerConfig(\n",
        "                          vocab=DEFAULT_VOCAB\n",
        "                      ),\n",
        "                  },\n",
        "              )\n",
        "          ),\n",
        "      ],\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi_ZZDOlhSWE"
      },
      "source": [
        "Retrieve the set of sample records at each transformation step.\n",
        "\n",
        "By default, 2 sample records are retrieved from the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVevI4gZhoSE"
      },
      "outputs": [],
      "source": [
        "steps = task.get_dataset_by_step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMbQ9HY-ns_x"
      },
      "source": [
        "The length of the returned list indicates the number of steps that were recorded:\n",
        "\n",
        "    |---------------------------|\n",
        "    | 0 | Raw data              |\n",
        "    | 1 | _imdb_preprocessor()  |\n",
        "    | 2 | tokenizer.tokenize()  |\n",
        "    |---------------------------|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqU87YbnnZqQ"
      },
      "outputs": [],
      "source": [
        "len(steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32SJFB5jo29q"
      },
      "source": [
        "View the keys at each step to see how fields are transformed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9YVnqj5pAOZ"
      },
      "outputs": [],
      "source": [
        "for i, step in enumerate(steps):\n",
        "  print(f\"Step {i}: {step[0].keys()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTPS2KZ5JflU"
      },
      "source": [
        "View the records at each step to see how raw data is transformed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g_kJiUinYAi"
      },
      "outputs": [],
      "source": [
        "for i, step in enumerate(steps):\n",
        "  print(f\"--Step {i}-----------------------\")\n",
        "  for element in step:\n",
        "    for k, v in element.items():\n",
        "      print(f\"    {k}: {v}\")\n",
        "    print(f\"  -------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNqr8j5NJ9P9"
      },
      "source": [
        "Now that we understand how the raw data is transformed, we can run transformations on the full dataset. If we were to load deterministically, the first few records would be equivalent to the values after the last step above (`steps[:-1]`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiqERc5bolpA"
      },
      "outputs": [],
      "source": [
        "ds = task.get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2INd2V0QKIXO"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for element in ds:\n",
        "  for k, v in element.items():\n",
        "    print(f\"    {k}: {v}\")\n",
        "  print(f\"  -------------------------\")\n",
        "  count += 1\n",
        "  if count \u003e= dataset_providers.DEFAULT_NUM_RECORDS_TO_INSPECT:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v33VaOTMyBS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {},
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
