{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApA2WBvW4qjB"
      },
      "source": [
        "# AirIO Train with WMT Example\n",
        "\n",
        "This notebook demonstrates training with T5X on the [WMT](https://www.tensorflow.org/datasets/catalog/wmt19_translate) dataset. It performs the following actions:\n",
        "\n",
        "* Creates an AirIO task that handles:\n",
        "  * Loading the dataset\n",
        "  * Mapping raw data to a format suitable for training\n",
        "  * Tokenizing the text using SeqIO's [`SentencePieceVocabulary`](https://github.com/google/seqio/blob/main/seqio/vocabularies.py)\n",
        "* Defines a small T5 1.1 model\n",
        "* Defines a function for training\n",
        "\n",
        "The training function is called, performing 3 steps of training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEUrg5cgZjbN"
      },
      "source": [
        "## Imports and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xvFqesO4hSk"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import functools\n",
        "import tempfile\n",
        "\n",
        "from airio import examples\n",
        "from seqio import vocabularies\n",
        "from t5x import adafactor\n",
        "from t5x import gin_utils\n",
        "from t5x import models\n",
        "from t5x import partitioning\n",
        "from t5x import train as train_lib\n",
        "from t5x import trainer\n",
        "from t5x import utils\n",
        "from t5x.examples.t5 import network\n",
        "\n",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cgIEkge6F9_"
      },
      "outputs": [],
      "source": [
        "_DEFAULT_EXTRA_IDS = 100\n",
        "_DEFAULT_SPM_PATH = \"gs://t5-data/vocabs/cc_all.32000/sentencepiece.model\"\n",
        "_DEFAULT_VOCAB = vocabularies.SentencePieceVocabulary(\n",
        "    _DEFAULT_SPM_PATH, _DEFAULT_EXTRA_IDS\n",
        ")\n",
        "_EVAL_STEPS = 2\n",
        "_SOURCE_SEQUENCE_LENGTH = 32\n",
        "_TOTAL_STEPS = 3\n",
        "_WORKDIR = tempfile.mkdtemp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrkSuPzLaNz_"
      },
      "source": [
        "## Define a small T5 1.1 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw8sQf2f6TiD"
      },
      "outputs": [],
      "source": [
        "def get_t5_model(**config_overrides) -\u003e models.EncoderDecoderModel:\n",
        "  \"\"\"Returns a small T5 1.1 model.\"\"\"\n",
        "  tiny_config = network.T5Config(\n",
        "      vocab_size=32128,\n",
        "      dtype=\"bfloat16\",\n",
        "      emb_dim=8,\n",
        "      num_heads=4,\n",
        "      num_encoder_layers=2,\n",
        "      num_decoder_layers=2,\n",
        "      head_dim=3,\n",
        "      mlp_dim=16,\n",
        "      mlp_activations=(\"gelu\", \"linear\"),\n",
        "      dropout_rate=0.0,\n",
        "      logits_via_embedding=False,\n",
        "  )\n",
        "  tiny_config = dataclasses.replace(tiny_config, **config_overrides)\n",
        "  return models.EncoderDecoderModel(\n",
        "      module=network.Transformer(tiny_config),\n",
        "      input_vocabulary=_DEFAULT_VOCAB,\n",
        "      output_vocabulary=_DEFAULT_VOCAB,\n",
        "      optimizer_def=adafactor.Adafactor(\n",
        "          decay_rate=0.8,\n",
        "          step_offset=0,\n",
        "          logical_factor_rules=adafactor.standard_logical_factor_rules(),\n",
        "      ),\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOsn7ZFPagDl"
      },
      "source": [
        "## Define a function for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji9ERpI76WHk"
      },
      "outputs": [],
      "source": [
        "def create_train_fn(task: dataset_providers.GrainTask):\n",
        "  \"\"\"Returns a callable function for training.\"\"\"\n",
        "  train_dataset_cfg = utils.DatasetConfig(\n",
        "      mixture_or_task_name=task,\n",
        "      task_feature_lengths={\n",
        "          \"inputs\": _SOURCE_SEQUENCE_LENGTH,\n",
        "          \"targets\": _SOURCE_SEQUENCE_LENGTH\n",
        "      },\n",
        "      split=\"train\",\n",
        "      batch_size=8,\n",
        "      shuffle=False,\n",
        "      pack=False,\n",
        "      use_cached=False,\n",
        "      seed=0,\n",
        "  )\n",
        "  eval_dataset_cfg = utils.DatasetConfig(\n",
        "      mixture_or_task_name=task,\n",
        "      task_feature_lengths={\n",
        "          \"inputs\": _SOURCE_SEQUENCE_LENGTH,\n",
        "          \"targets\": _SOURCE_SEQUENCE_LENGTH\n",
        "      },\n",
        "      split=\"validation\",\n",
        "      batch_size=8,\n",
        "      shuffle=False,\n",
        "      pack=False,\n",
        "      use_cached=False,\n",
        "      seed=0,\n",
        "  )\n",
        "  partitioner = partitioning.PjitPartitioner(num_partitions=4)\n",
        "  trainer_cls = functools.partial(\n",
        "      trainer.Trainer,\n",
        "      learning_rate_fn=utils.create_learning_rate_scheduler(\n",
        "          factors=\"constant * rsqrt_decay\",\n",
        "          base_learning_rate=1.0,\n",
        "          warmup_steps=1000,\n",
        "      ),\n",
        "      num_microbatches=None,\n",
        "  )\n",
        "  restore_cfg = None\n",
        "  ckpt_cfg = utils.CheckpointConfig(\n",
        "      save=utils.SaveCheckpointConfig(\n",
        "          dtype=\"float32\",\n",
        "          period=4,\n",
        "          checkpoint_steps=[0, 1, 2, 3, 4, 80, 97, 100],\n",
        "      ),\n",
        "      restore=restore_cfg,\n",
        "  )\n",
        "  return functools.partial(\n",
        "      train_lib.train,\n",
        "      model=get_t5_model(),\n",
        "      train_dataset_cfg=train_dataset_cfg,\n",
        "      train_eval_dataset_cfg=eval_dataset_cfg,\n",
        "      infer_eval_dataset_cfg=None,\n",
        "      checkpoint_cfg=ckpt_cfg,\n",
        "      partitioner=partitioner,\n",
        "      trainer_cls=trainer_cls,\n",
        "      total_steps=_TOTAL_STEPS,\n",
        "      eval_steps=_EVAL_STEPS,\n",
        "      eval_period=1000,\n",
        "      random_seed=0,\n",
        "      summarize_config_fn=gin_utils.summarize_gin_config,\n",
        "      use_orbax=False,\n",
        "      gc_period=4,\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D5CYlGUakCk"
      },
      "source": [
        "## Create a Task.\n",
        "\n",
        "Create an AirIO task that handles dataset loading, raw data mapping, and tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk7xKWTK6Ya9"
      },
      "outputs": [],
      "source": [
        "wmt_task = examples.tasks.get_wmt_19_ende_v003_task()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QepXIps_a5ll"
      },
      "source": [
        "## Create a training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUoBqy9f6cVx"
      },
      "outputs": [],
      "source": [
        "train_fn = create_train_fn(wmt_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mffDoaWCbEPk"
      },
      "source": [
        "## Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBscCxvZbEeV"
      },
      "outputs": [],
      "source": [
        "step, _ = train_fn(model_dir=_WORKDIR)\n",
        "print(f\"step: {step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3wLuVaowwxr"
      },
      "source": [
        "## Visualize training with TensorBoard\n",
        "\n",
        "Load the extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUj6jfwREwhF"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tsmqpKCxD6q"
      },
      "source": [
        "Launch the UI. Note the metrics for `train` and `training_eval`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgL9sOUia_nJ"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=$_WORKDIR --port=0"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {},
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
