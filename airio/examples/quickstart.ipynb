{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xknXK2DRQEf3"
      },
      "source": [
        "# AirIO Quickstart Example\n",
        "\n",
        "This notebook demonstrates the creation of a basic `Task` with two\n",
        "preprocessing steps. It performs the following actions:\n",
        "\n",
        "1. Load the [IMDB reviews][imdb_reviews] dataset.\n",
        "2. Map the raw data to a format suitable for training.\n",
        "3. Tokenize the text using SeqIO's\n",
        "[`SentencePieceVocabulary`][seqio_vocabularies].\n",
        "\n",
        "The task's `get_dataset()` method is called, to demonstrate the contents\n",
        "of each record after all transformation steps.\n",
        "\n",
        "\n",
        "[imdb_reviews]: https://www.tensorflow.org/datasets/catalog/imdb_reviews\n",
        "[seqio_vocabularies]: https://github.com/google/seqio/blob/main/seqio/vocabularies.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyEzJh40P0LQ"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "import airio\n",
        "from seqio import vocabularies\n",
        "\n",
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4BMqxKTQLHm"
      },
      "source": [
        "First we load a SeqIO vocabulary for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1IXwcMJQMn_"
      },
      "outputs": [],
      "source": [
        "DEFAULT_SPM_PATH = \"gs://t5-data/vocabs/mc4.250000.100extra/sentencepiece.model\"\n",
        "DEFAULT_VOCAB = vocabularies.SentencePieceVocabulary(DEFAULT_SPM_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd8o0KyEQOn4"
      },
      "source": [
        "The raw data contains two fields per record: `{\"text\", \"label\"}`. For training, we need alternate field names: `{\"inputs\", \"targets\"}`. The values in each record need to be a suitable format, so we first define a function for converting raw data.\n",
        "\n",
        "This function performs the following:\n",
        "* Remaps the field names to `{\"inputs\", \"targets\"}`\n",
        "* Adds the prefix `\"imdb \"` to the raw text\n",
        "* Maps the raw label from an integer to `{\"negative\", \"positive\", \"invalid\"}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeGZUDFjQQf0"
      },
      "outputs": [],
      "source": [
        "def _imdb_preprocessor(raw_example: Dict[str, bytes]) -\u003e Dict[str, str]:\n",
        "    final_example = {\"inputs\": \"imdb \" + raw_example[\"text\"].decode(\"utf-8\")}\n",
        "    raw_label = str(raw_example[\"label\"])\n",
        "    if raw_label == \"0\":\n",
        "      final_example[\"targets\"] = \"negative\"\n",
        "    elif raw_label == \"1\":\n",
        "      final_example[\"targets\"] = \"positive\"\n",
        "    else:\n",
        "      final_example[\"targets\"] = \"invalid\"\n",
        "    return final_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLEydeDDQSHz"
      },
      "source": [
        "Next we define a task that uses this function as a preprocessor, followed by tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NZo5R1pQVES"
      },
      "outputs": [],
      "source": [
        "task = airio.dataset_providers.Task(\n",
        "      name=\"dummy_airio_task\",\n",
        "      source=airio.data_sources.TfdsDataSource(\n",
        "          tfds_name=\"imdb_reviews/plain_text:1.0.0\", splits=[\"train\"]\n",
        "      ),\n",
        "      preprocessors=[\n",
        "          airio.preprocessors.MapFnTransform(_imdb_preprocessor),\n",
        "          airio.preprocessors.MapFnTransform(\n",
        "              airio.tokenizer.Tokenizer(\n",
        "                  tokenizer_configs={\n",
        "                      \"inputs\": airio.tokenizer.TokenizerConfig(\n",
        "                          vocab=DEFAULT_VOCAB\n",
        "                      ),\n",
        "                      \"targets\": airio.tokenizer.TokenizerConfig(\n",
        "                          vocab=DEFAULT_VOCAB\n",
        "                      ),\n",
        "                  },\n",
        "              )\n",
        "          ),\n",
        "      ],\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e4npoX6QZSE"
      },
      "source": [
        "Now we can retrieve an iterator to view the effect of the series of transformations on the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3zeSHr1QdGL"
      },
      "outputs": [],
      "source": [
        "ds = task.get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hwxg1830Qdo0"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for element in ds:\n",
        "  for k, v in element.items():\n",
        "    print(f\"    {k}: {v}\")\n",
        "  print(f\"  -------------------------\")\n",
        "  count += 1\n",
        "  if count \u003e= airio.dataset_providers.DEFAULT_NUM_RECORDS_TO_INSPECT:\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {},
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
